{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324ed650",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f34ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "#findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671e5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME='/opt/cloudera/parcels/CDH/lib/spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffe51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c535d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, Mar 23 2020, 22:36:06) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys; print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab65e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "#from hdfs import InsecureClient\n",
    "import numpy as np\n",
    "#from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import rtree\n",
    "from pyspark.sql import Window\n",
    "#import igraph\n",
    "#from igraph import Graph\n",
    "import geofeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28ff493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import LongType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD\n",
    "from sedona.core.SpatialRDD import PointRDD\n",
    "from sedona.core.SpatialRDD import PolygonRDD\n",
    "from sedona.core.SpatialRDD import LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e601ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"./env/bin/python\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf\"\n",
    "\n",
    "# 2) Force PySpark to use THIS kernel's Python\n",
    "import os, sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1becc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop an existing session/context without creating a new one\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark import SparkContext\n",
    "if getattr(SparkContext, \"_active_spark_context\", None) is not None:\n",
    "    SparkContext._active_spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa4cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.jars        -> /local/data/ruichen/icespark/spark/sedona-core-2.4_2.11-1.0.0-incubating.jar,/local/data/ruichen/icespark/spark/sedona-sql-2.4_2.11-1.0.0-incubating.jar,/local/data/ruichen/icespark/spark/sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,/local/data/ruichen/icespark/spark/sedona-viz-2.4_2.11-1.0.0-incubating.jar,/local/data/ruichen/icespark/spark/geotools-wrapper-geotools-24.0.jar\n",
      "spark.yarn.jars   -> local:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars/*,local:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/hive/*\n",
      "driver.classpath  -> None\n",
      "executor.classpath-> None\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"test_test_4\")\n",
    "    .master(\"yarn\")\n",
    "    # ---- Sedona/Kryo (use strings) ----\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    # ---- JARs: ABSOLUTE paths ----\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/local/data/ruichen/icespark/spark/sedona-core-2.4_2.11-1.0.0-incubating.jar,\"\n",
    "        \"/local/data/ruichen/icespark/spark/sedona-sql-2.4_2.11-1.0.0-incubating.jar,\"\n",
    "        \"/local/data/ruichen/icespark/spark/sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,\"\n",
    "        \"/local/data/ruichen/icespark/spark/sedona-viz-2.4_2.11-1.0.0-incubating.jar,\"\n",
    "        \"/local/data/ruichen/icespark/spark/geotools-wrapper-geotools-24.0.jar\"\n",
    "    )\n",
    "\n",
    "    # ---- Packaged conda env (if you’re using conda-pack) ----\n",
    "    .config(\"spark.yarn.dist.archives\",\n",
    "            \"/local/data/ruichen/icespark/spark/environment.tar.gz#environment\")\n",
    "    .config(\"spark.pyspark.python\", \"./environment/bin/python\")\n",
    "    .config(\"spark.pyspark.driver.python\", \"./environment/bin/python\")\n",
    "    # ---- Resources & misc ----\n",
    "    .config(\"spark.executor.memory\", \"20g\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6144\")\n",
    "    .config(\"spark.executor.instances\", \"24\")\n",
    "    .config(\"spark.executor.cores\", \"5\")\n",
    "#     .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "#     .config(\"spark.blacklist.killBlacklistedExecutors\", \"true\")\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Make Spark interpret timestamps in UTC so we match GeoPandas utc=True\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# sanity print\n",
    "conf = dict(spark.sparkContext.getConf().getAll())\n",
    "print(\"spark.jars        ->\", conf.get(\"spark.jars\"))\n",
    "print(\"spark.yarn.jars   ->\", conf.get(\"spark.yarn.jars\"))\n",
    "print(\"driver.classpath  ->\", conf.get(\"spark.driver.extraClassPath\"))\n",
    "print(\"executor.classpath->\", conf.get(\"spark.executor.extraClassPath\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd62819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162825bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-01-01'\n",
    "end_date = '2023-10-31'\n",
    "# duration_month = 6 # for export\n",
    "# time_index_hour_length = 24\n",
    "# hour_delay = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9932a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir -p /user/wangr/icespark/\n",
    "# hdfs dfs -put /local/data/ruichen/icespark/L8_PS_2023/l8_metadata_export_wkt.csv /user/wangr/icespark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bfeed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|id                  |timestamp          |l8_timestamp       |\n",
      "+--------------------+-------------------+-------------------+\n",
      "|LC08_031008_20230401|2023-04-01 17:15:09|2023-04-01 17:15:09|\n",
      "+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")  # keep\n",
    "\n",
    "l8_df_raw = (\n",
    "    spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", False)   # IMPORTANT: keep strings\n",
    "      .load(\"hdfs:///user/wangr/icespark/l8_metadata_export_wkt.csv\")  # your path\n",
    ")\n",
    "\n",
    "l8_df_raw = l8_df_raw.withColumn(\n",
    "    \"l8_timestamp\",\n",
    "    F.coalesce(\n",
    "        F.to_timestamp(F.col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss[.SSS]X\"),\n",
    "        F.to_timestamp(F.col(\"timestamp\"))  # fallback\n",
    "    )\n",
    ")\n",
    "\n",
    "# sanity check this exact ID\n",
    "l8_df_raw.filter(F.col(\"id\") == \"LC08_031008_20230401\") \\\n",
    "  .select(\"id\", \"timestamp\", \"l8_timestamp\") \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da5a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to put these two lines in command line to copy files over\n",
    "# hdfs dfs -mkdir -p /user/wangr/L8_PS_2023/60N_byMonth \n",
    "# hdfs dfs -put /local/data/ruichen/icespark/L8_PS_2023/60N_byMonth/*.csv /user/wangr/L8_PS_2023/60N_byMonth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388a9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_raw = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", False)\n",
    "        .option(\"multiLine\", True)             # fields can span lines\n",
    "        .option(\"quote\", '\"')                  # quoted fields\n",
    "        .option(\"escape\", '\"')                 # \"\" inside quotes\n",
    "        .option(\"maxCharsPerColumn\", 10_000_000)\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .option(\"recursiveFileLookup\", True)   # read subdirs too\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")     # only CSVs\n",
    "#         .load(\"file:///local/data/ruichen/icespark/L8_PS_2023/60N_byMonth/\")                            # reads/concats all matching files\n",
    "        .load(\"hdfs:///user/wangr/L8_PS_2023/60N_byMonth/*.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d59b81db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------------+\n",
      "|id                     |capture_time                    |\n",
      "+-----------------------+--------------------------------+\n",
      "|20230401_203607_13_2426|2023-04-01 20:36:07.136725+00:00|\n",
      "+-----------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_df_raw.filter(F.col(\"id\") == \"20230401_203607_13_2426\") \\\n",
    "  .select(\"id\", \"capture_time\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "333262b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1699601"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b122fc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18128011"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0ca05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l8_df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a216fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7bd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5d58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78f8c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L8 rows (after date filter): 135807\n",
      "+--------------------+-------------------+\n",
      "|l8_id               |l8_timestamp       |\n",
      "+--------------------+-------------------+\n",
      "|LC08_001004_20230330|2023-03-30 14:08:10|\n",
      "|LC08_001004_20230415|2023-04-15 14:07:56|\n",
      "|LC08_001004_20230704|2023-07-04 14:08:02|\n",
      "|LC08_001004_20230720|2023-07-20 14:08:04|\n",
      "|LC08_001004_20230805|2023-08-05 14:08:14|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fix weird column name -> \"geometry\"\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fix weird column name\n",
    "l8_df_raw = l8_df_raw.withColumnRenamed(\"geometry\\r\", \"geometry\")\n",
    "\n",
    "# Build a clean L8 Sedona dataframe with ONE correct UTC timestamp\n",
    "l8_df_sedona = (\n",
    "    l8_df_raw\n",
    "      .select(\n",
    "          F.col(\"id\").alias(\"l8_id\"),\n",
    "          F.col(\"geometry\").alias(\"l8_wkt\"),\n",
    "\n",
    "          # treat timestamp as STRING explicitly\n",
    "          F.col(\"timestamp\").cast(\"string\").alias(\"l8_timestamp_raw\"),\n",
    "      )\n",
    "\n",
    "      # geometry from WKT\n",
    "      .withColumn(\"l8_geometry\", F.expr(\"ST_GeomFromWKT(l8_wkt)\"))\n",
    "\n",
    "      # Explicit ISO-8601 parsing (UTC)\n",
    "      .withColumn(\n",
    "          \"l8_timestamp\",\n",
    "          F.coalesce(\n",
    "              F.to_timestamp(\"l8_timestamp_raw\", \"yyyy-MM-dd'T'HH:mm:ss[.SSS]X\"),\n",
    "              F.to_timestamp(\"l8_timestamp_raw\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "          )\n",
    "      )\n",
    "\n",
    "      # drop raw helpers\n",
    "      .drop(\"l8_wkt\", \"l8_timestamp_raw\")\n",
    ")\n",
    "\n",
    "# === Date range filter (now safe and consistent) ===\n",
    "l8_df_sedona = l8_df_sedona.filter(\n",
    "    (F.to_date(\"l8_timestamp\") >= F.lit(start_date)) &\n",
    "    (F.to_date(\"l8_timestamp\") <= F.lit(end_date))\n",
    ")\n",
    "\n",
    "# === Sanity check ===\n",
    "print(\"L8 rows (after date filter):\", l8_df_sedona.count())\n",
    "l8_df_sedona.select(\"l8_id\", \"l8_timestamp\").show(5, truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a0ffc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS rows (after date filter): 17886985\n",
      "PS timestamp NULLs: 0\n",
      "+-----------------------+--------------------------+\n",
      "|ps_id                  |ps_timestamp              |\n",
      "+-----------------------+--------------------------+\n",
      "|20230719_225128_31_2276|2023-07-19 22:51:28.316055|\n",
      "|20230731_231309_06_2490|2023-07-31 23:13:09.06451 |\n",
      "|20230731_231306_91_2490|2023-07-31 23:13:06.917345|\n",
      "|20230731_221304_07_2460|2023-07-31 22:13:04.070628|\n",
      "|20230731_221306_20_2460|2023-07-31 22:13:06.205436|\n",
      "+-----------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a clean PS Sedona dataframe with UTC timestamps\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# fix weird geometry column if present\n",
    "if \"geometry\\r\" in ps_df_raw.columns and \"geometry\" not in ps_df_raw.columns:\n",
    "    ps_df_raw = ps_df_raw.withColumnRenamed(\"geometry\\r\", \"geometry\")\n",
    "\n",
    "# choose correct id column\n",
    "id_col = \"id\" if \"id\" in ps_df_raw.columns else \"ps_id\"\n",
    "\n",
    "ps_df_sedona = (\n",
    "    ps_df_raw\n",
    "      .select(\n",
    "          F.col(id_col).alias(\"ps_id\"),\n",
    "          F.col(\"geometry\").alias(\"ps_wkt\"),\n",
    "          F.col(\"acquired\").cast(\"string\").alias(\"ps_ts_raw\"),\n",
    "      )\n",
    "      .withColumn(\"ps_geometry\", F.expr(\"ST_GeomFromWKT(ps_wkt)\"))\n",
    "      .withColumn(\n",
    "          \"ps_timestamp\",\n",
    "          F.coalesce(\n",
    "              F.to_timestamp(\"ps_ts_raw\", \"yyyy-MM-dd'T'HH:mm:ss[.SSS]X\"),\n",
    "              F.to_timestamp(\"ps_ts_raw\", \"yyyy-MM-dd HH:mm:ss\"),\n",
    "              F.to_timestamp(\"ps_ts_raw\")  # last-resort fallback\n",
    "          )\n",
    "      )\n",
    "      .drop(\"ps_wkt\", \"ps_ts_raw\")\n",
    ")\n",
    "\n",
    "# date filter\n",
    "ps_df_sedona = ps_df_sedona.filter(\n",
    "    (F.to_date(\"ps_timestamp\") >= F.lit(start_date)) &\n",
    "    (F.to_date(\"ps_timestamp\") <= F.lit(end_date))\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "print(\"PS rows (after date filter):\", ps_df_sedona.count())\n",
    "print(\"PS timestamp NULLs:\", ps_df_sedona.filter(F.col(\"ps_timestamp\").isNull()).count())\n",
    "ps_df_sedona.select(\"ps_id\", \"ps_timestamp\").show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "259e991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps_df_sedona.filter(F.col(\"ps_id\") == \"20230401_203607_13_2426\") \\\n",
    "#   .select(\"ps_id\", \"ps_timestamp\") \\\n",
    "#   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "350cf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the area for the l8\n",
    "\n",
    "# l8_df_sedona.createOrReplaceTempView(\"l8_df_sedona\")\n",
    "# l8_df_sedona = spark.sql(\"select ST_Area(l8_geometry) as l8_area, * from l8_df_sedona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad233d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the area for the ps\n",
    "\n",
    "# ps_df_sedona.createOrReplaceTempView(\"ps_df_sedona\")\n",
    "# ps_df_sedona = spark.sql(\"select ST_Area(ps_geometry) as ps_area, * from ps_df_sedona\")\n",
    "\n",
    "# ps_df_sedona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61db108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"\\n========== PRE-JOIN DIAGNOSTICS ==========\")\n",
    "\n",
    "# # Count records\n",
    "# ps_count = ps_df_sedona.count()\n",
    "# l8_count = l8_df_sedona.count()\n",
    "# print(f\"PS count: {ps_count}\")\n",
    "# print(f\"L8 count: {l8_count}\")\n",
    "\n",
    "# # Date ranges\n",
    "# print(\"\\nPS date range:\")\n",
    "# ps_df_sedona.agg(\n",
    "#     F.min(\"ps_timestamp\").alias(\"min_date\"),\n",
    "#     F.max(\"ps_timestamp\").alias(\"max_date\")\n",
    "# ).show(truncate=False)\n",
    "\n",
    "# print(\"L8 date range:\")\n",
    "# l8_df_sedona.agg(\n",
    "#     F.min(\"l8_timestamp\").alias(\"min_date\"),\n",
    "#     F.max(\"l8_timestamp\").alias(\"max_date\")\n",
    "# ).show(truncate=False)\n",
    "\n",
    "# # Monthly distribution\n",
    "# print(\"\\nPS by month:\")\n",
    "# ps_df_sedona.withColumn(\"month\", F.date_format(\"ps_timestamp\", \"yyyy-MM\")) \\\n",
    "#     .groupBy(\"month\").count().orderBy(\"month\").show(20, truncate=False)\n",
    "\n",
    "# print(\"L8 by month:\")\n",
    "# l8_df_sedona.withColumn(\"month\", F.date_format(\"l8_timestamp\", \"yyyy-MM\")) \\\n",
    "#     .groupBy(\"month\").count().orderBy(\"month\").show(20, truncate=False)\n",
    "\n",
    "# # Check for nulls\n",
    "# print(\"\\nNull check:\")\n",
    "# print(f\"PS nulls - geometry: {ps_df_sedona.filter(F.col('ps_geometry').isNull()).count()}, \"\n",
    "#       f\"timestamp: {ps_df_sedona.filter(F.col('ps_timestamp').isNull()).count()}\")\n",
    "# print(f\"L8 nulls - geometry: {l8_df_sedona.filter(F.col('l8_geometry').isNull()).count()}, \"\n",
    "#       f\"timestamp: {l8_df_sedona.filter(F.col('l8_timestamp').isNull()).count()}\")\n",
    "\n",
    "# print(\"==========================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61520edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'(big.date_for_join = small.date_for_join)'>]\n",
      "t0: 9007906.952515692\n",
      "t1: 9008084.470621962\n",
      "[Broadcasted l8 + date_for_join index (±1 day on reference) + unsalted] rows=10051, time=177.52s\n",
      "Distinct PS: 8499, Distinct L8: 1222\n",
      "+-----------------------+--------------------+--------------------------+-------------------+---------+\n",
      "|ps_id                  |l8_id               |ps_ts                     |l8_ts              |time_diff|\n",
      "+-----------------------+--------------------+--------------------------+-------------------+---------+\n",
      "|20230716_011750_99_2451|LC08_094241_20230716|2023-07-16 01:17:50.998998|2023-07-16 01:17:21|0.008333 |\n",
      "|20230917_220836_00_248f|LC08_078016_20230917|2023-09-17 22:08:36.002381|2023-09-17 22:09:06|0.008333 |\n",
      "|20230714_045456_00_2495|LC08_144013_20230714|2023-07-14 04:54:56.002625|2023-07-14 04:55:26|0.008333 |\n",
      "|20230426_022038_01_2414|LC08_119014_20230426|2023-04-26 02:20:38.011212|2023-04-26 02:21:08|0.00833  |\n",
      "|20230708_215504_98_24ba|LC08_061244_20230708|2023-07-08 21:55:04.985641|2023-07-08 21:54:35|0.008329 |\n",
      "|20230504_193240_97_2483|LC08_038244_20230504|2023-05-04 19:32:40.979483|2023-05-04 19:32:11|0.008328 |\n",
      "|20230811_034218_97_249a|LC08_132014_20230811|2023-08-11 03:42:18.970791|2023-08-11 03:41:49|0.008325 |\n",
      "|20231006_061229_96_2478|LC08_156018_20231006|2023-10-06 06:12:29.965255|2023-10-06 06:12:00|0.008324 |\n",
      "|20230409_212614_95_240c|LC08_071017_20230409|2023-04-09 21:26:14.950665|2023-04-09 21:25:45|0.00832  |\n",
      "|20230901_220843_94_240c|LC08_078014_20230901|2023-09-01 22:08:43.947132|2023-09-01 22:08:14|0.008319 |\n",
      "+-----------------------+--------------------+--------------------------+-------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Saved to: /local/data/ruichen/icespark/st_join_log.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# --- knobs ---\n",
    "seconds = 30 \n",
    "smaller_side = \"l8\"    # \"l8\" = treat L8 as reference (like Sentinel in Peiqi's script); \"ps\" = PS as reference\n",
    "use_salting = False    # set True to enable salting on the large side\n",
    "salt_buckets = 8       # replication factor for the smaller side on hot keys (2–16 is typical)\n",
    "\n",
    "# Convert seconds window to an integer hour \"delay\" for day shifting\n",
    "# (same idea as Peiqi: day-level index with ±1 day padding; final filter is still ±seconds)\n",
    "hour_delay = max(1, (seconds + 3599) // 3600)  # ceil(seconds/3600), but at least 1\n",
    "\n",
    "# --- helpful configs (AQE etc.; some are no-ops on Spark 2.4 but harmless) ---\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 256 * 1024 * 1024)  # 256 MB\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "\n",
    "# 1) Column pruning\n",
    "ps_small = ps_df_sedona.select(\"ps_id\", \"ps_geometry\", \"ps_timestamp\")\n",
    "l8_small = l8_df_sedona.select(\"l8_id\", \"l8_geometry\", \"l8_timestamp\")\n",
    "\n",
    "# Decide which dataset is the \"reference\" (Peiqi's Sentinel) and which is the \"other\" (Peiqi's IS2)\n",
    "if smaller_side == \"l8\":\n",
    "    ref_df = (\n",
    "        l8_small\n",
    "          .withColumn(\"hour\", F.hour(\"l8_timestamp\"))\n",
    "          .withColumn(\"date\", F.to_date(\"l8_timestamp\"))\n",
    "    )\n",
    "    other_df = ps_small.withColumn(\"date_for_join\", F.to_date(\"ps_timestamp\"))\n",
    "    ref_ts_col_name = \"l8_timestamp\"\n",
    "    other_ts_col_name = \"ps_timestamp\"\n",
    "    ref_id_col = \"l8_id\"\n",
    "    other_id_col = \"ps_id\"\n",
    "else:\n",
    "    ref_df = (\n",
    "        ps_small\n",
    "          .withColumn(\"hour\", F.hour(\"ps_timestamp\"))\n",
    "          .withColumn(\"date\", F.to_date(\"ps_timestamp\"))\n",
    "    )\n",
    "    other_df = l8_small.withColumn(\"date_for_join\", F.to_date(\"l8_timestamp\"))\n",
    "    ref_ts_col_name = \"ps_timestamp\"\n",
    "    other_ts_col_name = \"l8_timestamp\"\n",
    "    ref_id_col = \"ps_id\"\n",
    "    other_id_col = \"l8_id\"\n",
    "\n",
    "# 2) Build temporal index on the reference side, mirroring Peiqi's date_for_join logic\n",
    "#    - \"center\": same day\n",
    "#    - \"late\":   hour >= 23 - hour_delay -> date_for_join = date + 1 day\n",
    "#    - \"early\":  hour < hour_delay       -> date_for_join = date - 1 day\n",
    "\n",
    "ref_late = ref_df.filter(F.col(\"hour\") >= (23 - hour_delay))\n",
    "ref_early = ref_df.filter(F.col(\"hour\") < hour_delay)\n",
    "\n",
    "ref_center = ref_df.withColumn(\"date_for_join\", F.col(\"date\"))\n",
    "ref_late   = ref_late.withColumn(\"date_for_join\", F.date_add(F.col(\"date\"), 1))\n",
    "ref_early  = ref_early.withColumn(\"date_for_join\", F.date_sub(F.col(\"date\"), 1))\n",
    "\n",
    "# Union them together (same schema)\n",
    "ref_indexed = (\n",
    "    ref_center\n",
    "      .unionByName(ref_late)\n",
    "      .unionByName(ref_early)\n",
    ")\n",
    "\n",
    "# ref_indexed.show(100)\n",
    "# other_df.show(100)\n",
    "# print(ref_indexed.count())\n",
    "\n",
    "# 3) Optional skew salting (on the large side)\n",
    "#    - Large side gets a hash-based 'salt'\n",
    "#    - Reference (small) side is replicated across all salt buckets\n",
    "if use_salting:\n",
    "    # Decide which is large vs small based on smaller_side\n",
    "    if smaller_side == \"l8\":\n",
    "        # L8 is reference (small), PS is large\n",
    "        big_df = other_df.withColumn(\n",
    "            \"salt\",\n",
    "            F.pmod(F.hash(F.col(other_id_col)), F.lit(salt_buckets))\n",
    "        )\n",
    "        small_df = ref_indexed.withColumn(\n",
    "            \"salt\",\n",
    "            F.explode(F.sequence(F.lit(0), F.lit(salt_buckets - 1)))\n",
    "        )\n",
    "    else:\n",
    "        # PS is reference (small), L8 is large\n",
    "        big_df = other_df.withColumn(\n",
    "            \"salt\",\n",
    "            F.pmod(F.hash(F.col(other_id_col)), F.lit(salt_buckets))\n",
    "        )\n",
    "        small_df = ref_indexed.withColumn(\n",
    "            \"salt\",\n",
    "            F.explode(F.sequence(F.lit(0), F.lit(salt_buckets - 1)))\n",
    "        )\n",
    "    join_keys = [\"date_for_join\", \"salt\"]\n",
    "else:\n",
    "    big_df = other_df\n",
    "    small_df = ref_indexed\n",
    "    join_keys = [\"date_for_join\"]\n",
    "\n",
    "# 4) Repartition the large side by join keys for locality (like Peiqi's temporal index join)\n",
    "big_df = big_df.repartition(*[F.col(k) for k in join_keys])\n",
    "\n",
    "# 5) Build join (broadcast reference side) + spatial + tight time window\n",
    "big_alias = big_df.alias(\"big\")\n",
    "small_alias = broadcast(small_df).alias(\"small\")\n",
    "\n",
    "# Join condition: equality on temporal index (and salt if enabled)\n",
    "join_cond = [F.col(\"big.\" + k) == F.col(\"small.\" + k) for k in join_keys]\n",
    "print(join_cond)\n",
    "\n",
    "# Geometry + timestamp columns for downstream expressions\n",
    "ps_ts_col = (\n",
    "    F.col(\"big.ps_timestamp\") if smaller_side == \"l8\" else F.col(\"small.ps_timestamp\")\n",
    ")\n",
    "l8_ts_col = (\n",
    "    F.col(\"small.l8_timestamp\") if smaller_side == \"l8\" else F.col(\"big.l8_timestamp\")\n",
    ")\n",
    "\n",
    "if smaller_side == \"l8\":\n",
    "    spatial_expr = F.expr(\"ST_Within(big.ps_geometry, small.l8_geometry)\")\n",
    "\n",
    "else:\n",
    "    spatial_expr = F.expr(\"ST_Within(big.ps_geometry, small.l8_geometry)\")\n",
    "\n",
    "    \n",
    "\n",
    "t0 = time.perf_counter()\n",
    "print(f\"t0: {t0}\")\n",
    "\n",
    "\n",
    "joined = (\n",
    "    big_alias\n",
    "      .join(small_alias, on=join_cond, how=\"inner\")   # temporal index join on date_for_join (+ salt)\n",
    "      .where(spatial_expr)                            # spatial predicate\n",
    "      # compute exact time difference in seconds\n",
    "      .withColumn(\n",
    "          \"delta_sec\",\n",
    "          F.abs(ps_ts_col.cast(\"double\") - l8_ts_col.cast(\"double\"))\n",
    "      )\n",
    "      .where(F.col(\"delta_sec\") <= seconds)\n",
    "      # now select final columns, including time_diff in hours if you want it\n",
    "      .select(\n",
    "          (F.col(\"big.ps_id\") if smaller_side == \"l8\" else F.col(\"small.ps_id\")).alias(\"ps_id\"),\n",
    "          (F.col(\"small.l8_id\") if smaller_side == \"l8\" else F.col(\"big.l8_id\")).alias(\"l8_id\"),\n",
    "          ps_ts_col.alias(\"ps_ts\"),\n",
    "          l8_ts_col.alias(\"l8_ts\"),\n",
    "#           F.col(\"delta_sec\").alias(\"time_diff\")\n",
    "          F.round(\n",
    "              F.col(\"delta_sec\") / 3600.0, 6\n",
    "          ).alias(\"time_diff\"),   # still in hours for reporting\n",
    "      )\n",
    "      .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "\n",
    "# print(\"\\n=== Physical plan for joined (Peiqi-style temporal index) ===\")\n",
    "# joined.explain(True)\n",
    "# print(\"=== End physical plan ===\\n\")\n",
    "\n",
    "# 6) Trigger & measure\n",
    "total = joined.count()\n",
    "t1 = time.perf_counter()\n",
    "print(f\"t1: {t1}\")\n",
    "print(\n",
    "    f\"[Broadcasted {smaller_side} + date_for_join index \"\n",
    "    f\"(±1 day on reference) + {'salted' if use_salting else 'unsalted'}] \"\n",
    "    f\"rows={total}, time={t1 - t0:.2f}s\"\n",
    ")\n",
    "\n",
    "# 7) Final counts\n",
    "row = joined.agg(\n",
    "    F.countDistinct(\"ps_id\").alias(\"distinct_ps\"),\n",
    "    F.countDistinct(\"l8_id\").alias(\"distinct_l8\")\n",
    ").first()\n",
    "print(f\"Distinct PS: {row['distinct_ps']}, Distinct L8: {row['distinct_l8']}\")\n",
    "\n",
    "\n",
    "joined.orderBy(F.col(\"time_diff\").desc()).show(10, truncate=False)\n",
    "\n",
    "joined.unpersist()\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "msg = (f\"[{ts}] [Temporal-indexed join, ±{seconds}s] rows={total}, \"\n",
    "       f\"Distinct PS: {row['distinct_ps']}, Distinct L8: {row['distinct_l8']}, \"\n",
    "       f\"time={t1 - t0:.2f}s\")\n",
    "\n",
    "log_path = \"/local/data/ruichen/icespark/st_join_log.txt\"\n",
    "Path(log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(msg + \"\\n\")\n",
    "print(\"Saved to:\", log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "213866cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK_OUT = f\"spark_matches_{seconds}s\"\n",
    "# print(SPARK_OUT)\n",
    "\n",
    "# joined.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(SPARK_OUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7be7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -ls /user/wangr/spark_matches_30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43829626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -getmerge \\\n",
    "#   /user/wangr/spark_matches_30s\\\n",
    "#   /local/data/ruichen/icespark/spark_matches_30s.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22131a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ps_one = ps_df_sedona.filter(F.col(\"ps_id\") == \"20230409_225916_09_240c\") \\\n",
    "#     .select(\"ps_id\", \"ps_geometry\", \"ps_timestamp\")\n",
    "\n",
    "# l8_one = l8_df_sedona.filter(F.col(\"l8_id\") == \"LC08_087015_20230409\") \\\n",
    "#     .select(\"l8_id\", \"l8_geometry\", \"l8_timestamp\")\n",
    "\n",
    "# pair_check = (\n",
    "#     ps_one.crossJoin(l8_one)\n",
    "#       .withColumn(\"intersects\", F.expr(\"ST_Intersects(ps_geometry, l8_geometry)\"))\n",
    "#       .withColumn(\"delta_sec\", F.abs(F.col(\"ps_timestamp\").cast(\"long\") - F.col(\"l8_timestamp\").cast(\"long\")))\n",
    "#       .select(\"ps_id\", \"l8_id\", \"ps_timestamp\", \"l8_timestamp\", \"intersects\", \"delta_sec\")\n",
    "# )\n",
    "\n",
    "# pair_check.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a6fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
